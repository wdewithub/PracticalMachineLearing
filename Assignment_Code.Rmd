---
title: "PracticalMachineLearning_Assignment"
author: "wdewit"
date: "November 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r change working data, include=FALSE}
setwd(dir='c:/Users/ntpuser3/datascience/PracticalMachineLearing')
library(caret)
```

#Executive Summary 

Six participants wearing accelerometers on the belt, forearm, arm and dumbell were asked to perform barbell lifting in 5 different ways: 1 correct way and 4 common mistakes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
Each exercise was repeated 10 times. In this paper we try to find a set of variables predicting in which of the 6 ways they performed the exercise. 


#Data Exploration

The training dataset contains 19 622 observations on 160 variables. We start in first deleting the variables which have too little variation to explain anything on how well the participants performed the exercise. We also removed the row index variable at the beginning of the file and the timestamp variables.Finally, we removed the variables which only had values for a couple of observations. In doing so we kept 54 variables available for predicting the manner in which the participant did the exercise, recorded in the classe variable.  

```{r read and first view on data}
training <- read.csv(file="pml-training.csv", header=TRUE)
dim(training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
useful_metrics <- nsv$nzv==FALSE
useful_metrics_names <- names(training[useful_metrics])
training_lim <- training[,useful_metrics_names]
rm(useful_metrics, useful_metrics_names)
training_lim2 <- training_lim[,-c(1,3:6)]
check_missing <- function(x) {sum(is.na(x))}
mvalue_count <- apply(training_lim2,2,check_missing)
useful_metric_names <- names(mvalue_count[mvalue_count==0])
training_lim3 <- training_lim2[,useful_metric_names]
dim(training_lim3)
rm(training, training_lim, training_lim2, nsv, useful_metric_names)
```

Next we will first perform a clustering analysis to get a feeling how well we will be able to predict the classe variable. We choose k-means since the number of observations is nearly 20K which would require quite some memory to calculate the distances between observations when taking hierarchical clustering. However, even with 100 random starts k-means does not seem to separate classes well. 

```{r cluster analysis}
kClust <- kmeans(training_lim3[,2:53],centers=5, nstart=100)
table(kClust$cluster,training_lim3$classe)
```

Therefore we perform a singular value decomposition and plot the first left singular value with different colours by the classe variable. We see that the 5 classes are pretty neatly separated. So, we should be able to find a predictive model.In checking which variables load highly on the first right singular value or principal component (abs value > 0.2), we find 6 variables being responsible. So, they should appear as significant predictors in our final model. However, given that the first principal component only explains 16% of the variability in our dataset we could expect quite some more predictors to be important.

```{r singular value decomp}
svdecomp <- svd(scale(training_lim3[,2:53]))
par(mfrow=c(1,2))
plot(svdecomp$u[,1],col=training_lim3$classe)
plot(svdecomp$v[,1])
impvar <- abs(svdecomp$v[,1]) > 0.2
names(training_lim3[,2:53])[impvar]
pca <- prcomp(scale(training_lim3[,2:53]))
summary(pca)
```

#Formal modelling
Since we have 54 variables to evaluate on their predictive power we choose to work with a random forest which will create several bootstrap samples from our observations and sample during model building the variables, leading to a diverse set of decision trees being generated. In order to reduce overfitting we will combine random forest with a 5-fold cross-validation.  


```{r random forest}
library(e1071)
modfit <- train(classe ~ . - user_name, training_lim3, method="rf", trControl=trainControl(method="cv", number=5),prox=TRUE, allowparallell=TRUE)

```
Rode laptop heeft onvoldoende geheugen voor rf fitting

Eventueel hierboven ook eerstzoeken naar optimale mtyr en dan rf fitten met die mtyr:
zie http://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/
