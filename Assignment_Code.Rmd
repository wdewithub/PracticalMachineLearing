---
title: "PracticalMachineLearning_Assignment"
author: "wdewit"
date: "November 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r change working data, include=FALSE}
setwd(dir='C:/Users/Wendy/DataScience/PracticalMachineLearing/PracticalMachineLearing')
library(caret)
```

#Executive Summary 

Six participants wearing accelerometers on the belt, forearm, arm and dumbell were asked to perform barbell lifting in 5 different ways: 1 correct way and 4 common mistakes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Each exercise was repeated 10 times.

In this paper we try to find a set of variables predicting in which of the 5 ways they performed the exercise. Since the focus was on accurate prediction we fitted a random forest. We first determined the optimal number or variables to evaluate at each split. Next we focussed our efforts in building a less complex tree, both to improve calculation speed and to reduce the risk of overfitting. Our final random forest included 19 input variables, which promised us a good out of sample accuracy based on 3-fold cross-validation (accuracy =95.6%).

#Data Exploration

The training dataset contains 19 622 observations on 160 variables. We start in first deleting the variables which have too little variation to explain anything on how well the participants performed the exercise. We also removed the row index variable at the beginning of the file and the timestamp variables. Finally, we removed the variables which only had values for a couple of observations. In doing so we kept 52 variables available for predicting the manner in which the participant did the exercise, recorded in the classe variable.  

```{r read and first view on data}
training <- read.csv(file="pml-training.csv", header=TRUE)
dim(training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
useful_metrics <- nsv$nzv==FALSE
useful_metrics_names <- names(training[useful_metrics])
training_lim <- training[,useful_metrics_names]
rm(useful_metrics, useful_metrics_names)
training_lim2 <- training_lim[,-c(1,3:6)]
check_missing <- function(x) {sum(is.na(x))}
mvalue_count <- apply(training_lim2,2,check_missing)
useful_metric_names <- names(mvalue_count[mvalue_count==0])
training_lim3 <- training_lim2[,useful_metric_names]
dim(training_lim3)
rm(training, training_lim, training_lim2, nsv, useful_metric_names)
```

Next we will first perform a clustering analysis to get a feeling how well we will be able to predict the classe variable. We choose k-means since the number of observations is nearly 20K which would require quite some memory to calculate the distances between observations when taking hierarchical clustering. However, even with 100 random starts k-means does not seem to separate classes well. 

```{r cluster analysis}
kClust <- kmeans(training_lim3[,2:53],centers=5, nstart=100)
table(kClust$cluster,training_lim3$classe)
```

Therefore we perform a singular value decomposition and plot the first left singular value with different colours by the classe variable. We see that the 5 classes are pretty neatly separated. So, we should be able to find a predictive model.In checking which variables load highly on the first right singular value or principal component (abs value > 0.2), we find 6 variables being responsible. So, they should appear as significant predictors in our final model. However, given that the first principal component only explains 16% of the variability in our dataset we could expect quite some more predictors to be important.

```{r singular value decomp}
svdecomp <- svd(scale(training_lim3[,2:53]))
par(mfrow=c(1,2))
plot(svdecomp$u[,1],col=training_lim3$classe)
plot(svdecomp$v[,1])
impvar <- abs(svdecomp$v[,1]) > 0.2
names(training_lim3[,2:53])[impvar]
pca <- prcomp(scale(training_lim3[,2:53]))
summary(pca)
```

#Formal modelling

Since we have 52 variables, likely to have many high correlations, we choose to work with a random forest which will create several bootstrap samples from our observations and sample during model building the variables, leading to a diverse set of decision trees being generated. In order to reduce overfitting we will combine random forest with a 3-fold cross-validation. 3-fold cross-validation is not a lot, but given the size of our dataset both in observations and variables to evaluate a more stringent cross-validation would lead to inacceptably high processing times. With 52 inputvariables the default number of variables sampled at each split is floor(sqrt(52)) or 7. We will perform a grid search first to see whether any of the values between 4 and 14 produces a better accuracy. The highest accuracy is attained by randomly sampling 10 variables as candidates for each decision tree split. The out of sample error rate is estimated at 1-0.9941=0.59%.

```{r random forest, eval=FALSE}
library(e1071)
set.seed(1384)
tunegrid <- expand.grid(.mtry=c(4:14))
modfit_mtyrsearch <- train(classe ~ . - user_name, training_lim3, method="rf", trControl=trainControl(method="cv", number=3),prox=TRUE, allowparallell=TRUE,tuneGrid=tunegrid, nodesize=196)
print(modfit_mtyrsearch)
plot(modfit_mtyrsearch)
```

We continue using the random forest with mtyr=10. We will further avoiding overfitting in building a less deep tree in requiring that each node has at least 1% of the observations. With a 3-fold cross validation this comes down to 196/3=65. Fitting less complex trees increases the expected out of sample error rate to (1-0.9711)=2.89%, which is still very good.

```{r random forest fit with mtyr 10}
set.seed(1384)
mtry <- 10
tunegrid <- expand.grid(.mtry=mtry)
modfit <- train(classe ~ . - user_name, training_lim3, method="rf", trControl=trainControl(method="cv", number=3),prox=TRUE, allowparallell=TRUE,tuneGrid=tunegrid, nodesize=65)
print(modfit)
saveRDS(modfit$finalModel, "./final_model.rds")
```


Finally, we want to have a quick look which of our 52 variables are most important in predicting the Classe variable. We notice that 7 variables are very helpful in predicting the classe variable. From magnet_dumbbell_x onwards the importance drops with one third already. We decide to refit a random forest model with all variables having an importance > 200. This keeps 19 predictors, including 4 of the 6 variables which were found important by the singular value decomposition in the exploratory phase. 

```{r variable importance}
 finalmodel <- readRDS("./final_model.rds")
 varimp_raw <- varImp(finalmodel)
 varimp <- varimp_raw[order(varimp_raw[,1],decreasing=TRUE),, drop=FALSE]
 par(mfrow=c(1,1))
 barplot(varimp$Overall,names.arg=rownames(varimp),las=2,ylab='Variable importance')
```

In fitting a random forest based on the 19 most important predictors found earlier, our expected out of sample accuracy further lowered to 95.6%, which is still very acceptable. 

```{r more parsimonious model}
varimp200 <- varimp>200
varimp200_names <- c("classe",rownames(varimp)[varimp200])
training_limfinal <- training_lim3[,varimp200_names]
modfit_pars <- train(classe ~ . , training_limfinal, method="rf", trControl=trainControl(method="cv", number=3),prox=TRUE, allowparallell=TRUE,tuneGrid=tunegrid, nodesize=65)
print(modfit_pars)
saveRDS(modfit_pars$finalModel, "./final_model_pars.rds")
```

From the confusion matrix below we see that all of the classes have a high accuracy. Therefore we accept the random forest based on 19 predictors as the final model with an expected out of sample error rate of 1-95.6=4.4%

```{r checking accuracy individual classes}
predicted<- predict(modfit_pars$finalModel,training_limfinal)
table(predicted,training_limfinal$classe)
```