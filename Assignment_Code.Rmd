---
title: "PracticalMachineLearning_Assignment"
author: "wdewit"
date: "November 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r change working data, include=FALSE}
setwd(dir='C:/Users/Wendy/DataScience/PracticalMachineLearing/PracticalMachineLearing')
library(caret)
```

#Executive Summary 

Six participants wearing accelerometers on the belt, forearm, arm and dumbell were asked to perform barbell lifting in 5 different ways: 1 correct way and 4 common mistakes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
Each exercise was repeated 10 times. In this paper we try to find a set of variables predicting in which of the 6 ways they performed the exercise. Since the focus was on accurate prediction we fitted a random forest on 52 inputvariables. We were able to obtain a very high accuracy (> 99%).

#Data Exploration

The training dataset contains 19 622 observations on 160 variables. We start in first deleting the variables which have too little variation to explain anything on how well the participants performed the exercise. We also removed the row index variable at the beginning of the file and the timestamp variables. Finally, we removed the variables which only had values for a couple of observations. In doing so we kept 52 variables available for predicting the manner in which the participant did the exercise, recorded in the classe variable.  

```{r read and first view on data}
training <- read.csv(file="pml-training.csv", header=TRUE)
dim(training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
useful_metrics <- nsv$nzv==FALSE
useful_metrics_names <- names(training[useful_metrics])
training_lim <- training[,useful_metrics_names]
rm(useful_metrics, useful_metrics_names)
training_lim2 <- training_lim[,-c(1,3:6)]
check_missing <- function(x) {sum(is.na(x))}
mvalue_count <- apply(training_lim2,2,check_missing)
useful_metric_names <- names(mvalue_count[mvalue_count==0])
training_lim3 <- training_lim2[,useful_metric_names]
dim(training_lim3)
rm(training, training_lim, training_lim2, nsv, useful_metric_names)
```

Next we will first perform a clustering analysis to get a feeling how well we will be able to predict the classe variable. We choose k-means since the number of observations is nearly 20K which would require quite some memory to calculate the distances between observations when taking hierarchical clustering. However, even with 100 random starts k-means does not seem to separate classes well. 

```{r cluster analysis}
kClust <- kmeans(training_lim3[,2:53],centers=5, nstart=100)
table(kClust$cluster,training_lim3$classe)
```

Therefore we perform a singular value decomposition and plot the first left singular value with different colours by the classe variable. We see that the 5 classes are pretty neatly separated. So, we should be able to find a predictive model.In checking which variables load highly on the first right singular value or principal component (abs value > 0.2), we find 6 variables being responsible. So, they should appear as significant predictors in our final model. However, given that the first principal component only explains 16% of the variability in our dataset we could expect quite some more predictors to be important.

```{r singular value decomp}
svdecomp <- svd(scale(training_lim3[,2:53]))
par(mfrow=c(1,2))
plot(svdecomp$u[,1],col=training_lim3$classe)
plot(svdecomp$v[,1])
impvar <- abs(svdecomp$v[,1]) > 0.2
names(training_lim3[,2:53])[impvar]
pca <- prcomp(scale(training_lim3[,2:53]))
summary(pca)
```

#Formal modelling

Since we have 52 variables to evaluate on their predictive power we choose to work with a random forest which will create several bootstrap samples from our observations and sample during model building the variables, leading to a diverse set of decision trees being generated. In order to reduce overfitting we will combine random forest with a 3-fold cross-validation. With 52 inputvariables the default number of variables sampled at each split is floor(sqrt(52)) or 7. We will perform a grid search first to see whether any of the values between 4 and 14 produces a better accuracy. The highest accuracy is attained by randomly sampling 10 variables as candidates for each decision tree split. We continue using the random forest with mtyr=10 as the final model. The out of sample error rate is estimated at 1-0.9941=0.59%.

```{r random forest, cache=TRUE}
library(e1071)
set.seed(1384)
tunegrid <- expand.grid(.mtry=c(4:14))
modfit_mtyrsearch <- train(classe ~ . - user_name, training_lim3, method="rf", trControl=trainControl(method="cv", number=3),prox=TRUE, allowparallell=TRUE,tuneGrid=tunegrid)
print(modfit_mtyrsearch)
plot(modfit_mtyrsearch)
saveRDS(modfit_mtyrsearch$finalModel, "./final_model.rds")
```

Finally, we want to have a quick look which of our 52 variables are most important in predicting the Classe variable. We notice that 7 variables are very helpful in predicting the classe variable. From magnet_dumbbell_y onwards the importance drops with one third already. If a parsimonious model was important one might consider to fit a logistic regression model solely with those 7 predictors. However, as we focus on prediction accuracy we conclude our exercise with the random forest approach. 

```{r variable importance}
 varimp_raw <- varImp(modfit_mtyrsearch$finalModel)
 varimp <- varimp_raw[order(varimp_raw[,1],decreasing=TRUE),, drop=FALSE]
 barplot(varimp$Overall)
```

