plot(concrete$CompressiveStrength,col=concrete$CoarseAggregate)
plot(concrete$CompressiveStrength,col=concrete$FineAggregate)
plot(concrete$CompressiveStrength,col=concrete$Age)
plot(concrete$CompressiveStrength)
hist(training$Superplasticizer)
set.seed(3433)
data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis,p=0.75)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
str(testing)
varlijst <- grepl('IL',names(training))
varlijst
input <- training[,varlijst]
str(input)
varlijst <- grepl(^'IL',names(training))
varlijst <- grepl('^IL',names(training))
input <- training[,varlijst]
str(input)
preProcess(input,method="pca",tresh=0.90)
preProcess(input,method="pca",tresh=90)
pc <- preProcess(input,method="pca",tresh=0.90)
predict(pc,training)
str(predict(pc,training)
)
str(input)
str(predict(pc,input))
pc <- preProcess(input,method="pca",tresh=0.90)
pc
pc[1]
summary(pc)
pc <- princomp(input)
summary(pc)
pred <- predict(pc,input)
plot(pred)
inputtest <- testing[,varlijst]
pred <- predict(pc,inputtest)
summary(pred)
pred
pred$dev
pred$sdev
pc <- preProcess(input,method="pca",tresh=0.90)
summary(pc)
pc
pc <- preProcess(input,method="pca",tresh=0.80)
pc
pc <- preProcess(input,method="pca",tresh=0.8)
plot(predict(pc,inputtest))
pctest <- predict(pc,inputtest)
pca <- princomp(input)
summary(pca)
set.seed(3433)
data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain <- createDataPartition(adData$diagnosis,p=0.75)[[1]]
training <- adData[inTrain,]
testing <- adData[-inTrain,]
input <- training[,grepl('^IL',names(training))]
names(input)
input <- cbind(input,training$diagnosis)
names(input)
head(input)
modfit <- train(input[,13]~.,method="glm", data=input)
install.packages("e1071")
library(e1071)
modfit <- train(input[,13]~.,method="glm", data=input)
input[,13]
modfit <- train(input[,13]~input[,1:12],method="glm", data=input)
modfit <- train(input[[,13]]~input[[,1:12]],method="glm", data=input)
str(input)
modfit <- train(input[[,13]]~input$IL_11+input$IL_13+input$IL_16,method="glm", data=input)
modfit <- train(input$`training$diagnosis`~input$IL_11+input$IL_13+input$IL_16,method="glm", data=input)
modfit <- train(`training$diagnosis`~.,method="glm", data=input)
confusionMatrix(testing$diagnosis,predict(modfit,testing))
pca80 <- preProcess(training,method="pca",tresh=0.8)
valuespca80 <- predict(pca80,training)
pca80 <- preProcess(training[,-'training$diagnosis'],method="pca",tresh=0.8)
names(training)
names(input)
modfit <- train(`training$diagnosis`~.,method="glm", data=input)
confusionMatrix(testing$diagnosis,predict(modfit,testing))
inputpca <- input[,1:12]
names(inputpca)
pca80 <- preProcess(inputpca,method="pca",tresh=0.8)
valuespca80 <- predict(pca80,inputpca)
pcamodelinput <- cbind(valuespca80,input[,13])
names(pcamodelinput)
names(pcamodelinput)[11] <- "diagnosis"
names(pcamodelinput)
modelfitpca <- train(diagnosis~.,method="glm",data=pcamodelinput)
names(testing)
inputtesting <- training[,grepl('^IL',names(testing))]
testingpca <- predict(pca80,inputtesting)
confusionMatrix(testing$diagnosis,predict(modfitpca,testingpca))
confusionMatrix(testing$diagnosis,predict(modelfitpca,testingpca))
length(testingpca)
predtest <- predict(modelfitpca,testingpca)
str(predtest)
length(predtest)
length(testing$diagnosis)
inputtesting <- testing[,grepl('^IL',names(testing))]
testingpca <- predict(pca80,inputtesting)
confusionMatrix(testing$diagnosis,predict(modelfitpca,testingpca))
set.seed(3433)
data(AlzheimerDisease)
adData <- data.frame(diagnosis,predictors)
input <- adData[,grepl('^IL',names(adData))]
names(input)
pc <- princomp(input)
summary(pc)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData=data.frame(diagnosis,predictors)
inTrain=createDataPartition(adData$diagnosis, p=3/4)[[1]]
training=adData[inTrain,]
testing=adData[-inTrain,]
pca80 <- preProcess(training,methode='pca',tresh=0.8)
pca80
str(pca80)
pca <- princomp(training)
pca <- prcomp(training)
str(training)
library(datasets)
str(iris)
irissub <- iris[,1:4]
preProcess(irrissub,method='pca',tresh=0.8)
preProcess(irissub,method='pca',tresh=0.8)
summary(prcomp(irissub))
library(MASS)
data(Boston)
set.seed(10457)
train.idx <- createDataPartition(Boston$medv,p=0.75,list=FALSE)
train <- Boston[train.idx,]
test <- Boston[-train.idx,]
preProc <- preProcess(train[,-14],method=c("center", "scale", "pca"), tresh=0.8)
summary(preProc)
train_pca <- predict(preProc,train[,-14])
head(train_pca)
preProc
preProc <- preProcess(train[,-14],method=c("center", "scale", "pca"), pcaComp=7)
preProc
summary(prcomp(train[,-14]))
trainsc <- scale(train)
head(train)
head(trainsc)
summary(prcomp(trainsc[,-14]))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
libary(caret)
library(caret)
library(caret)
set.seed=125
training <- createDataPartition(segmentationOriginal$Case)
traindata <- segmentationOriginal[training,]
testdata <- segmentationOriginal[-training,]
boom <- train(Case ~ ., data=traindata, method='rpart')
training <- createDataPartition(segmentationOriginal$Case, list=FALSE)
traindata <- segmentationOriginal[training,]
testdata <- segmentationOriginal[-training,]
boom <- train(Case ~ ., data=traindata, method='rpart')
install.packages("rattle")
library(rattle)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed=125
training <- createDataPartition(segmentationOriginal$Case, list=FALSE)
traindata <- segmentationOriginal[training,]
testdata <- segmentationOriginal[-training,]
boom <- train(Case ~ ., data=traindata, method='rpart')
library(rattle)
library(rattle)
fancyRpartPlot(boom$finalModel)
library(rattle)
fancyRpartPlot(boom$finalModel)
fancyRpartPlot(boom$finalModel)
install.packages("rpart.plot")
fancyRpartPlot(boom$finalModel)
set.seed=125
training <- createDataPartition(segmentationOriginal$Class, list=FALSE)
traindata <- segmentationOriginal[training,]
testdata <- segmentationOriginal[-training,]
boom <- train(Class ~ ., data=traindata, method='rpart')
library(rattle)
fancyRpartPlot(boom$finalModel)
levels(segmentationOriginal$Case)
table(segmentationOriginal$Case)
traindata <- segmentationOriginal[levels(Case)=="Train",]
testdata <- segmentationOriginal[levels(Case)=="Test",]
traindata <- segmentationOriginal[levels(segmentationOriginal$Case)=="Train",]
testdata <- segmentationOriginal[levels(segmentationOriginal$Case)=="Test",]
boom <- train(Class ~ ., data=traindata, method='rpart')
fancyRpartPlot(boom$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive=olive[,-1]
boomolive <- train(olive$Area ~., data=olive, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(boomolive,data=newdata)
boomolive <- train(olive$Area ~., data=olive, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(boomolive,data=newdata)
boomolive <- train(Area ~., data=olive, method="rpart")
predict(boomolive,data=newdata)
predict(boomolive,data=newdata)[1]
newdata
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(125)
traindata <- segmentationOriginal[levels(segmentationOriginal$Case)=="Train",]
testdata <- segmentationOriginal[levels(segmentationOriginal$Case)=="Test",]
boom <- train(Class ~ ., data=traindata, method='rpart')
library(rattle)
fancyRpartPlot(boom$finalModel)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F) #random sample uit 1:aantal obs, aantal keer=numobs/2, sample without replacement
trainSA = SAheart[train,]
testSA = SAheart[-train,]
modfit <- train(chd~age+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
modfit <- train(as.factor(chd)~age+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
misClass(trainSA,predict(modfit,trainSA))
misClass(testSA,predict(modfit,testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
misClass(trainSA,predict(modfit,trainSA))
misClass(testSA,predict(modfit,testSA))
load(misClass)
misClass(trainSA,predict(modfit,trainSA))
misClass(testSA,predict(modfit,testSA))
missClass(trainSA,predict(modfit,trainSA))
missClass(testSA,predict(modfit,testSA))
confusionMatrix(trainSa,predict(modfit,trainSA))
confusionMatrix(trainSA,predict(modfit,trainSA))
predict(modfit,trainSA)[1]
confusionMatrix(trainSA$chd,predict(modfit,trainSA))
missClass(trainSA$chd,predict(modfit,trainSA))
confusionMatrix(testSA$chd,predict(modfit,testSA))
data(vowel.train)
data(vowel.test)
head(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modfit <- train(y~., data=vowel.train, method="rf")
library(randomForest)
modfit <- train(y~., data=vowel.train, method="rf")
varImp(modfit)
varImp(predict(modfit,vowel.test))
predict(modfit,vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modfitrf <- train(y~., method="rf",data=vowel.train, prox=TRUE)
library(caret)
modfitrf <- train(y~., method="rf",data=vowel.train, prox=TRUE)
print(modfitrt$finalModel)
print(modfitrf$finalModel)
modfitboost <- train(y~., method="gbm", data=vowel.train, verbose="FALSE")
rint(modfitboost$finalModel)
print(modfitboost$finalModel)
confusionMatrix(vowel.train$y,mofitboost$finalModel)
confusionMatrix(vowel.train$y,modfitboost$finalModel)
confusionMatrix(vowel.train$y,predict(vowel.train$y,modfitboost))
print(modfitboost)
a <- predict(vowel.test,modfitrf)==predict(vowel.test,modfitboost)
head(a)
a <- predict(vowel.test,modfitrf)
a <- predict(modfitrf,vowel.test)==predict(modfitboost,vowel.test)
head(a)
sum(a)
mean(a)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modfitrf <- train(diagnosis~.,data=training, method="rf", prox=TRUE)
modfitgbm <- train(diagnosis~.,data=training, method="gbm", verbose=FALSE)
modfitlda <- train(diagnosis~.,data=training, method="lda")
predrf <- predict(modfitrf,testing)
predgbm <- predict(modfitgbm,testing)
predlda <- predict(modfitlda,testing)
predDF <- data.frame(predrf,predgbm,predlda,diagnosis=testing$diagnosis)
comboFit <- train(diagnosis~.,method="gam",data=predDF)
combPred <- predict(comboFit,predDF)
confusionMatrix(predrf,testing$diagnosis)
confusionMatrix(predgbm,testing$diagnosis)
confusionMatrix(predlda,testing$diagnosis)
confusionMatrix(combPred,testing$diagnosis)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
modfitlasso <- train(CompressiveStrength~.,data=training, method="lasso")
?plot.enet
plot.enet(modfitlasso,xvar=c("penalty"))
modfitlasso$coef
modfitlasso$results
modfitlasso$finalModel
dat = read.csv("C:/tmp/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
dat = read.csv("C:/tmp/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
str(dat)
year(dat$date)
?year
??year
install.packages("lubridate")
library(lubridate)
year(dat$date)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
str(training)
install.packages("forecast")
library(forecast)
modelts <- bats(tstrain)
dim(testing)
plot(forecast(modelts, level=95, h=dim(testing)))
fcts <- forecast(modelts, level=95, h=235)
head(fcts)
str(fcts)
withinboundaries <- testing$visitsTumblr <= fcts$upper & testing$visitsTumblr >= fcts$lower
mean(withinboundaries)
et.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
install.packages("e1071")
svmmodel <- svm(CompressiveStrength~.,data=training)
set.seed(325)
library(e1071)
svmmodel <- svm(CompressiveStrength~.,data=training)
svmpredict <- predict(svmmodel,testing)
svmpredict
svmpredict$finalModel
summary(svmpredict)
rmse <- sqrt(sum((svmpredict-testing$CompressiveStrength)^2))
rmse
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
svmmodel <- svm(CompressiveStrength~.,data=training)
svmpredict <- predict(svmmodel,testing)
rmse <- sqrt(sum((svmpredict-testing$CompressiveStrength)^2))
rmse
str(svmpredict)
rmse <- sqrt(mean((svmpredict-testing$CompressiveStrength)^2))
rmse
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
modfitlasso <- lars(CompressiveStrength~.)
modfitlasso$coef
modfitlasso$finalModel
install.packages("markdown")
library(markdown)
devtools::install_url("http://cran.r-project.org/src/contrib/rmarkdown_1.0.tar.gz")
install.packages("devtools")
devtools::install_url("http://cran.r-project.org/src/contrib/rmarkdown_1.0.tar.gz")
install.packages("rmarkdown", repos = "https://cran.revolutionanalytics.com")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: change working data
setwd(dir='c:/Users/ntpuser3/datascience/PracticalMachineLearing')
library(caret)
# Chunk 3: read and first view on data
training <- read.csv(file="pml-training.csv", header=TRUE)
dim(training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
useful_metrics <- nsv$nzv==FALSE
useful_metrics_names <- names(training[useful_metrics])
training_lim <- training[,useful_metrics_names]
rm(useful_metrics, useful_metrics_names)
training_lim2 <- training_lim[,-c(1,3:6)]
check_missing <- function(x) {sum(is.na(x))}
mvalue_count <- apply(training_lim2,2,check_missing)
useful_metric_names <- names(mvalue_count[mvalue_count==0])
training_lim3 <- training_lim2[,useful_metric_names]
dim(training_lim3)
rm(training, training_lim, training_lim2, nsv, useful_metric_names)
# Chunk 4: cluster analysis
kClust <- kmeans(training_lim3[,2:53],centers=5, nstart=100)
table(kClust$cluster,training_lim3$classe)
# Chunk 5: singular value decomp
svdecomp <- svd(scale(training_lim3[,2:53]))
par(mfrow=c(1,2))
plot(svdecomp$u[,1],col=training_lim3$classe)
plot(svdecomp$v[,1])
impvar <- abs(svdecomp$v[,1]) > 0.2
names(training_lim3[,2:53])[impvar]
pca <- prcomp(scale(training_lim3[,2:53]))
summary(pca)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: change working data
setwd(dir='C:/Users/Wendy/DataScience/PracticalMachineLearing/PracticalMachineLearing')
library(caret)
# Chunk 3: read and first view on data
training <- read.csv(file="pml-training.csv", header=TRUE)
dim(training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
useful_metrics <- nsv$nzv==FALSE
useful_metrics_names <- names(training[useful_metrics])
training_lim <- training[,useful_metrics_names]
rm(useful_metrics, useful_metrics_names)
training_lim2 <- training_lim[,-c(1,3:6)]
check_missing <- function(x) {sum(is.na(x))}
mvalue_count <- apply(training_lim2,2,check_missing)
useful_metric_names <- names(mvalue_count[mvalue_count==0])
training_lim3 <- training_lim2[,useful_metric_names]
dim(training_lim3)
rm(training, training_lim, training_lim2, nsv, useful_metric_names)
# Chunk 4: cluster analysis
kClust <- kmeans(training_lim3[,2:53],centers=5, nstart=100)
table(kClust$cluster,training_lim3$classe)
# Chunk 5: singular value decomp
svdecomp <- svd(scale(training_lim3[,2:53]))
par(mfrow=c(1,2))
plot(svdecomp$u[,1],col=training_lim3$classe)
plot(svdecomp$v[,1])
impvar <- abs(svdecomp$v[,1]) > 0.2
names(training_lim3[,2:53])[impvar]
pca <- prcomp(scale(training_lim3[,2:53]))
summary(pca)
library(randomForest)
opt_mtyr <- rfcv(training_lim3[,2:53], training_lim3$classe, step=0.8)
library(e1071)
tunegrid <- expand.grid(.mtry=c(4:17))
modfit_mtyrsearch <- train(classe ~ . - user_name, training_lim3, method="rf", trControl=trainControl(method="cv", number=3),prox=TRUE, allowparallell=TRUE,tuneGrid=tunegrid)
print(modfit_mtyrsearch)
plot(modfit_mtyrsearch)
modfit_mtyrsearch$times
varImp(modfit$FinalModel)
varImp(modfit_mtyrsearch$finalModel)
plot(varImp(modfit_mtyrsearch$finalModel))
order(varImp(modfit_mtyrsearch$finalModel))
sort(varImp(modfit_mtyrsearch$finalModel))
varimp_raw <- varImp(modfit_mtyrsearch$finalModel)
str(varimp_raw)
varimp_raw[order(varimp_raw$Overall)]
varimp_raw[order(varimp_raw[,1])]
varimp_raw[order(varimp_raw[,1]),]
names(varimp_raw)
varimp_raw
arrange(varimp_raw,varimp_raw[,1])
package(plyr)
install.packages("plyr")
library(plyr)
library(dplyr)
arrange(varimp_raw,varimp_raw[,1])
sort(varimp,decreasing=TRUE)
sort(varimp_raw,decreasing=TRUE)
sort(varimp_raw$Overall,decreasing=TRUE)
varimp_raw[order(varimp_raw[,1], drop=FALSE),]
varimp_raw[order(varimp_raw[,1]), drop=FALSE]
varimp_raw[order(varimp_raw[,1]),, drop=FALSE]
varimp <- varimp_raw[order(varimp_raw[,1]),, drop=FALSE]
barplot(varimp)
barplot(varimp$Overall)
saveRDS(modfit_mtyrsearch$final_model, "./final_model.rds")
testing <- read.csv(file="pml-testing.csv", header=TRUE)
dim(testing)
modfit_mtyrsearch$finalModel
dim(varimp_raw)
print(modfit_mtyrsearch)
testing <- read.csv(file="pml-testing.csv", header=TRUE)
dim(testing)
pred <- predict(modfit_mtyrsearch$finalModel,testing)
names(training_lim3)
testing_new <- testing[,names(training_lim3)]
columns <- names(training_lim3)
testing_new <- testing[,columns]
names(testing)
testing_new <- testing[,columns]
columns <- names(training_lim3[2:53])
testing_new <- testing[,columns]
pred <- predict(modfit_mtyrsearch$final_model,testing_new)
pred <- predict(modfit_mtyrsearch$finalModel,testing_new)
table(training_lim3$classe,pred)
dim(pred)
pred
